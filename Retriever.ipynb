{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Импорты успешно выполнены.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Подключение к OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "print(\"Импорты успешно выполнены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>seamus</td>\n",
       "      <td>after you, of course.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>ron</td>\n",
       "      <td>quiet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>ron</td>\n",
       "      <td>let the man through.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>ron</td>\n",
       "      <td>i didn't mean to open it, harry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>ron</td>\n",
       "      <td>it was badly wrapped.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>ron</td>\n",
       "      <td>they made me do it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>fred &amp; george</td>\n",
       "      <td>did not.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>george</td>\n",
       "      <td>it's a firebolt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>fred</td>\n",
       "      <td>it's the fastest broom in the world.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>harry</td>\n",
       "      <td>for me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>harry</td>\n",
       "      <td>but who sent it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>ron</td>\n",
       "      <td>no one knows.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>hermione</td>\n",
       "      <td>this came with it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>boy 1</td>\n",
       "      <td>go on, harry!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>boy 2</td>\n",
       "      <td>yeah, let's see.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>hermione</td>\n",
       "      <td>how fast is it, harry?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>harry</td>\n",
       "      <td>lumos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>harry</td>\n",
       "      <td>i solemnly swear that i am up to no good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>harry</td>\n",
       "      <td>mischief managed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4762</th>\n",
       "      <td>harry</td>\n",
       "      <td>nox.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          character                                   sentence\n",
       "4743         seamus                      after you, of course.\n",
       "4744            ron                                     quiet.\n",
       "4745            ron                       let the man through.\n",
       "4746            ron           i didn't mean to open it, harry.\n",
       "4747            ron                      it was badly wrapped.\n",
       "4748            ron                        they made me do it.\n",
       "4749  fred & george                                   did not.\n",
       "4750         george                           it's a firebolt.\n",
       "4751           fred       it's the fastest broom in the world.\n",
       "4752          harry                                    for me?\n",
       "4753          harry                           but who sent it?\n",
       "4754            ron                              no one knows.\n",
       "4755       hermione                         this came with it.\n",
       "4756          boy 1                              go on, harry!\n",
       "4757          boy 2                           yeah, let's see.\n",
       "4758       hermione                     how fast is it, harry?\n",
       "4759          harry                                     lumos.\n",
       "4760          harry  i solemnly swear that i am up to no good.\n",
       "4761          harry                          mischief managed.\n",
       "4762          harry                                       nox."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = []\n",
    "\n",
    "process = lambda text: text.strip().lower()\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    part = pd.read_csv(f\"Harry Potter {i+1}.csv\", sep=';')\n",
    "    part.columns = [*map(str.lower, part.columns)]\n",
    "\n",
    "    part.iloc[:, 0] = part.iloc[:, 0].apply(process)\n",
    "    part.iloc[:, 1] = part.iloc[:, 1].apply(process)\n",
    "\n",
    "    parts.append(part)\n",
    "\n",
    "all_parts_df = pd.concat(parts, axis=0)\n",
    "all_parts_df.drop_duplicates(inplace=True)\n",
    "all_parts_df.reset_index(drop=True, inplace=True)\n",
    "all_parts_df.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Возьмем реплики Гарри Поттера**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes, aunt petunia.',\n",
       " 'yes, uncle vernon.',\n",
       " \"he's asleep!\",\n",
       " 'sorry about him.',\n",
       " \"he doesn't understand what it's like, lying there day after day...\",\n",
       " '...watching people press their ugly faces in on you.',\n",
       " 'can you hear me?',\n",
       " \"it's just, i've never talked to a snake before.\",\n",
       " 'do you...?',\n",
       " 'i mean, do you talk to people often?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_replicas = all_parts_df[all_parts_df.character == \"harry\"].sentence.astype(str).tolist()\n",
    "harry_replicas[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Векторизацию будем проводить с помощью OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эмбеддинги загружены из файла.\n",
      "Размер массива эмбеддингов: (1028, 1536)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "texts_sample = harry_replicas\n",
    "\n",
    "embeddings_file = \"all_embeddings.npy\"\n",
    "\n",
    "if os.path.exists(embeddings_file):\n",
    "    all_embeddings = np.load(embeddings_file)\n",
    "    print(\"Эмбеддинги загружены из файла.\")\n",
    "else:\n",
    "    client = openai.OpenAI(api_key=openai.api_key)  # Новый способ вызова API\n",
    "\n",
    "    all_embeddings = []\n",
    "    for text in tqdm(texts_sample, desc=\"Embedding texts\"):\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"  # здесь размер эмбеддинга по дефолту 1536, это может быть избыточно для коротких фраз\n",
    "        )\n",
    "        emb = response.data[0].embedding\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    all_embeddings = np.array(all_embeddings, dtype='float32')\n",
    "    np.save(embeddings_file, all_embeddings)\n",
    "    print(\"Эмбеддинги вычислены и сохранены в файл.\")\n",
    "\n",
    "print(\"Размер массива эмбеддингов:\", all_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание индекса Faiss**\n",
    "\n",
    "##### Мы хотим искать близкие реплики по косинусному сходству. Faiss использует либо евклидово расстояние, либо скалярное произведение (dot product). Для косинусной близости можно сначала нормализовать вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего векторов в индексе: 1028\n"
     ]
    }
   ],
   "source": [
    "# Нормализуем\n",
    "norms = np.linalg.norm(all_embeddings, axis=1, keepdims=True)\n",
    "emb_normed = all_embeddings / norms\n",
    "\n",
    "dim = emb_normed.shape[1]\n",
    "\n",
    "# Создаём индекс для поиска по dot-product (что эквивалентно косинусному сходству после нормализации)\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "# Добавляем эмбеддинги в индекс\n",
    "index.add(emb_normed)\n",
    "\n",
    "print(f\"Всего векторов в индексе: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция для «поиска ответа»**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLY: and you? | SIMILARITY= 0.85968596\n",
      "REPLY: who are you? | SIMILARITY= 0.8581377\n",
      "REPLY: you okay? | SIMILARITY= 0.8550978\n",
      "REPLY: hello? | SIMILARITY= 0.85434425\n",
      "REPLY: hello? | SIMILARITY= 0.85434425\n"
     ]
    }
   ],
   "source": [
    "def get_closest_reply(user_text, top_k=1):\n",
    "    # 1) Эмбеддинг вопроса\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    resp = client.embeddings.create(input=user_text, model=\"text-embedding-ada-002\")\n",
    "    query_emb = np.array(resp.data[0].embedding, dtype='float32')\n",
    "    \n",
    "    # 2) Нормализуем\n",
    "    query_emb_normed = query_emb / np.linalg.norm(query_emb)\n",
    "    \n",
    "    # 3) Поиск в Faiss\n",
    "    D, I = index.search(query_emb_normed.reshape(1, -1), top_k)\n",
    "    # D - массив сходств, I - массив индексов\n",
    "    \n",
    "    # 4) Получим тексты\n",
    "    # texts_sample - это первые MAX_EMBED реплик, которые мы реально индексировали.\n",
    "    # Нужно брать по индексам из I.\n",
    "    \n",
    "    results = []\n",
    "    for rank in range(top_k):\n",
    "        idx = I[0][rank]\n",
    "        sim = D[0][rank]\n",
    "        text = texts_sample[idx]\n",
    "        results.append((text, sim))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Тестируем\n",
    "test_query = \"How are you?\"\n",
    "result = get_closest_reply(test_query, top_k=5)\n",
    "for r in result:\n",
    "    print(\"REPLY:\", r[0], \"| SIMILARITY=\", r[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Последние 2 ответа повторяются. В предобработке данных необходимо также проверить на дубли**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Коллега на занятии отметил, что правильно будет искать не близкую запросу реплику, а реплику на которую отвечает персонаж**\n",
    "\n",
    "##### Также преобразуем наш код в более правильную форму, так как почти все выше описанне действия включает в себя один компонент - Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего выбрано контекстов: 600\n",
      "Число запрошенных текстов: 600\n",
      "Число возвращённых эмбеддингов: 600\n"
     ]
    }
   ],
   "source": [
    "class OpenAIRetriever:\n",
    "    def __init__(self, model_name: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Инициализация ретривера с использованием OpenAI Embeddings.\n",
    "        \n",
    "        Параметры:\n",
    "        - model_name: имя модели OpenAI для получения эмбеддингов (по умолчанию \"text-embedding-ada-002\").\n",
    "        \n",
    "        Создается клиент OpenAI через конструкцию:\n",
    "            client = openai.OpenAI(api_key=openai.api_key)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        # Инициализируем клиента OpenAI\n",
    "        self.client = openai.OpenAI(api_key=openai.api_key)\n",
    "        self.contexts = []   # Список контекстов (реплики, на которые отвечает Гарри)\n",
    "        self.responses = []  # Список ответов Гарри (реплики, сказанные Гарри)\n",
    "        self.index = None    # Faiss индекс для контекстов\n",
    "        self.dim = None      # Размерность эмбеддингов\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"\n",
    "        При сериализации объекта исключает атрибут client.\n",
    "        \"\"\"\n",
    "        state = self.__dict__.copy()\n",
    "        del state['client']\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"\n",
    "        При десериализации объекта инициализирует атрибут client.\n",
    "        \"\"\"\n",
    "        self.__dict__.update(state)\n",
    "        self.client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n",
    "    def load_dialogue_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Обрабатывает DataFrame для формирования пар \"контекст–ответ\".\n",
    "        Если текущая реплика принадлежит Гарри, а предыдущая — нет, то:\n",
    "          - предыдущая реплика становится контекстом,\n",
    "          - текущая реплика становится ответом.\n",
    "          \n",
    "        После формирования пар вызывается load_corpus для индексирования контекстов. \n",
    "        \"\"\"\n",
    "        self.contexts.clear()\n",
    "        self.responses.clear()\n",
    "        \n",
    "        # Итерируем по DataFrame, начиная со второй строки\n",
    "        for i in range(1, len(df)):\n",
    "            prev_row = df.iloc[i - 1]\n",
    "            curr_row = df.iloc[i]\n",
    "            # Приводим имена персонажей к нижнему регистру и убираем лишние пробелы\n",
    "            if curr_row['character'].strip().lower() == 'harry' and prev_row['character'].strip().lower() != 'harry':\n",
    "                self.contexts.append(prev_row['sentence'])\n",
    "                self.responses.append(curr_row['sentence'])\n",
    "        print(f\"Всего выбрано контекстов: {len(self.contexts)}\")\n",
    "                \n",
    "        # После формирования пар индексируем контексты\n",
    "        self.load_corpus(self.contexts)\n",
    "\n",
    "    def load_corpus(self, contexts: list) -> None:\n",
    "        \"\"\"\n",
    "        Вычисляет эмбеддинги для списка контекстов с использованием OpenAI API и строит Faiss индекс.\n",
    "        Перед поиском эмбеддинги нормализуются, чтобы inner product работал как косинусное сходство.\n",
    "        \"\"\"\n",
    "        # Получаем эмбеддинги для контекстов\n",
    "        embeddings = self._get_embeddings(contexts)\n",
    "        # Нормализуем векторы (каждый вектор делим на его L2-норму)\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings_norm = embeddings / norms\n",
    "        \n",
    "        self.dim = embeddings_norm.shape[1]\n",
    "        # Создаем Faiss индекс с inner product (при нормализации это косинусное сходство)\n",
    "        self.index = faiss.IndexFlatIP(self.dim)\n",
    "        self.index.add(embeddings_norm.astype('float32'))\n",
    "\n",
    "    def _get_embeddings(self, texts: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Получает эмбеддинги для списка текстов через OpenAI API.\n",
    "        \n",
    "        texts: список строк.\n",
    "        \n",
    "        Возвращает numpy-массив эмбеддингов (dtype float32).\n",
    "        \"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=self.model_name\n",
    "        )\n",
    "        # Извлекаем эмбеддинги для каждого текста\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "\n",
    "        print(f\"Число запрошенных текстов: {len(texts)}\")\n",
    "        print(f\"Число возвращённых эмбеддингов: {len(response.data)}\")\n",
    "        return np.array(embeddings, dtype='float32')\n",
    "    \n",
    "    def encode_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Получает эмбеддинг для запроса пользователя и нормализует его.\n",
    "        \n",
    "        query: строка запроса.\n",
    "        \n",
    "        Возвращает эмбеддинг запроса в виде массива размерности (1, dim).\n",
    "        \"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            input=[query],\n",
    "            model=self.model_name\n",
    "        )\n",
    "        embedding = np.array(response.data[0].embedding, dtype='float32')\n",
    "        normalized = embedding / np.linalg.norm(embedding)\n",
    "        return normalized.reshape(1, -1)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> list:\n",
    "        \"\"\"\n",
    "        Выполняет поиск по запросу.\n",
    "        \n",
    "        1. Получает эмбеддинг запроса.\n",
    "        2. Ищет top_k наиболее похожих контекстов в Faiss индексе.\n",
    "        3. Возвращает список ответов (реплик Гарри), соответствующих найденным контекстам.\n",
    "        \n",
    "        query: строка запроса от пользователя.\n",
    "        top_k: количество результатов для возврата.\n",
    "        \"\"\"\n",
    "        query_embedding = self.encode_query(query)\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.responses):\n",
    "                results.append(self.responses[idx])\n",
    "        return results\n",
    "\n",
    "    def get_answer(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Возвращает лучший ответ (первый найденный) для заданного запроса.\n",
    "        Если результаты отсутствуют, возвращает сообщение-заглушку.\n",
    "        \"\"\"\n",
    "        results = self.search(query, top_k=1)\n",
    "        return results[0] if results else \"Извините, я не нашёл подходящего ответа.\"\n",
    "\n",
    "# Пример использования:\n",
    "# Допустим, у вас есть DataFrame all_parts_df с колонками 'character' и 'sentence'\n",
    "# (например, сформированный в результате объединения CSV файлов).\n",
    "retriever = OpenAIRetriever()\n",
    "retriever.load_dialogue_data(all_parts_df)\n",
    "# Сохранение объекта retriever в файл\n",
    "with open('retriever.pkl', 'wb') as file:\n",
    "    pickle.dump(retriever, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: hey, hagrid.\n"
     ]
    }
   ],
   "source": [
    "query = \"Harry, how are you?\"\n",
    "answer = retriever.get_answer(query)\n",
    "print(\"Ответ:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\betsu\\Miniconda3\\Lib\\site-packages\\gradio\\chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): обычно разрешается только одно использование адреса сокета (протокол/сетевой адрес/порт)\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7861): обычно разрешается только одно использование адреса сокета (протокол/сетевой адрес/порт)\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7862): обычно разрешается только одно использование адреса сокета (протокол/сетевой адрес/порт)\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7863): обычно разрешается только одно использование адреса сокета (протокол/сетевой адрес/порт)\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7864): обычно разрешается только одно использование адреса сокета (протокол/сетевой адрес/порт)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('retriever.pkl', 'rb') as file:\n",
    "    retriever = pickle.load(file)\n",
    "\n",
    "# Функция, которая отвечает на пользовательское сообщение, вызывая  retriever\n",
    "def chat_fn(user_message):\n",
    "    # Получаем ответ от ретривера\n",
    "    bot_answer = retriever.get_answer(user_message)  \n",
    "    return bot_answer\n",
    "\n",
    "#  Создание Gradio интерфейса\n",
    "chatbot = gr.ChatInterface(\n",
    "    fn=chat_fn,            # функция, которая будет обрабатывать новые сообщения\n",
    "    title=\"Гарри Поттер Бот\",\n",
    "    description=\"Задай вопрос боту и получи ответ от Гарри Поттера!\",\n",
    "    # Можно добавить несколько примеров (примеров ввода) для кнопок \"Examples\":\n",
    "    examples=[\"Hi, Harry!\", \"What do you think about magic?\"],\n",
    ")\n",
    "\n",
    "# Запуск локального сервера Gradio\n",
    "chatbot.launch(server_name=\"0.0.0.0\")  # Let Gradio choose a free port\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В общем получилось плохо. Долго мучился, но так и не понял, почему не получилось реализовать ответ. Возможно неудаяный датасет и выборка маленькая, всего 600 пар, где реплика Гарри следует за чьей-то другой**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
